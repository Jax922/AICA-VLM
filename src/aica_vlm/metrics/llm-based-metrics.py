import os
import sys
from openai import OpenAI
from dotenv import load_dotenv


def get_openai_client():
    load_dotenv()

    api_key = os.getenv("OPENAI_API_KEY")
    base_url = os.getenv("OPENAI_BASE_URL")
    if not api_key or not base_url:
        print("Missing OPENAI_API_KEY or OPENAI_BASE_URL in your .env file.")
        print("Please create a .env file with the following format:\n")
        print("OPENAI_API_KEY=your-api-key")
        print("OPENAI_BASE_URL=https://your-base-url")
        sys.exit(1)

    return OpenAI(api_key=api_key, base_url=base_url)

def get_deepseek_client():
    load_dotenv()

    api_key = os.getenv("DEEPSEEK_API_KEY")
    base_url = os.getenv("DEEPSEEK_BASE_URL")
    if not api_key or not base_url:
        print("Missing DEEPSEEK_API_KEY or DEEPSEEK_BASE_URL in your .env file.")
        print("Please create a .env file with the following format:\n")
        print("DEEPSEEK_API_KEY=your-api-key")
        print("DEEPSEEK_BASE_URL=https://your-base-url")
        sys.exit(1)

    return OpenAI(api_key=api_key, base_url=base_url)

def evaluate_emotion_reasoning(vlm_output, gpt4o_reference):
    """
    Evaluate metrics for Emotion Reasoning tasks by comparing VLM output with GPT-4o reference.

    Args:
        vlm_output (str): The output generated by the VLM model.
        gpt4o_reference (str): The reference answer generated by GPT-4o.

    Returns:
        dict: A dictionary containing scores for emotional alignment, descriptiveness,
              and causal soundness.
    """
    prompt = f"""
    You are an evaluator specializing in Emotion Reasoning tasks. The VLM output explains why a specific emotion is associated with the input (e.g., an image). Compare the VLM output with the GPT-4o reference and evaluate the following metrics:

    1. Emotional Alignment (1-5): Does the generated content match the target emotion?
    2. Descriptiveness (1-5): How detailed and vivid is the generated content?
    3. Causal Soundness (1-5): Does the explanation logically support the emotional inference?

    VLM Output:
    {vlm_output}

    GPT-4o Reference:
    {gpt4o_reference}

    Provide your evaluation in the following JSON format:
    {{
        "emotional_alignment": <score>,
        "descriptiveness": <score>,
        "causal_soundness": <score>
    }}
    """

    openai_client = get_openai_client()
    deepseek_client = get_deepseek_client()

    openai_response = openai_client.ChatCompletion.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "You are a helpful assistant for evaluating Emotion Reasoning tasks."},
            {"role": "user", "content": prompt}
        ]
    )

    deepseek_response = deepseek_client.ChatCompletion.create(
        model="deepseek",
        messages=[
            {"role": "system", "content": "You are a helpful assistant for evaluating Emotion Reasoning tasks."},
            {"role": "user", "content": prompt}
        ]
    )

    openai_evaluation = openai_response['choices'][0]['message']['content']
    deepseek_evaluation = deepseek_response['choices'][0]['message']['content']
    return openai_evaluation, deepseek_evaluation


def evaluate_emotion_guided_generation(vlm_output, gpt4o_reference):
    """
    Evaluate metrics for Emotion-Guided Content Generation tasks by comparing VLM output with GPT-4o reference.

    Args:
        vlm_output (str): The output generated by the VLM model.
        gpt4o_reference (str): The reference answer generated by GPT-4o.

    Returns:
        dict: A dictionary containing scores for emotional alignment and descriptiveness.
    """
    prompt = f"""
    You are an evaluator specializing in Emotion-Guided Content Generation tasks. The VLM output generates content guided by a target emotion. Compare the VLM output with the GPT-4o reference and evaluate the following metrics:

    1. Emotional Alignment (1-5): Does the generated content match the target emotion?
    2. Descriptiveness (1-5): How detailed and vivid is the generated content?

    VLM Output:
    {vlm_output}

    GPT-4o Reference:
    {gpt4o_reference}

    Provide your evaluation in the following JSON format:
    {{
        "emotional_alignment": <score>,
        "descriptiveness": <score>
    }}
    """

    openai_client = get_openai_client()
    deepseek_client = get_deepseek_client()


    openai_response = openai_client.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are a helpful assistant for evaluating Emotion-Guided Content Generation tasks."},
            {"role": "user", "content": prompt}
        ]
    )

    deepseek_response = deepseek_client.ChatCompletion.create(
        model="deepseek",
        messages=[
            {"role": "system", "content": "You are a helpful assistant for evaluating Emotion-Guided Content Generation tasks."},
            {"role": "user", "content": prompt}
        ]
    )
    openai_evaluation = openai_response['choices'][0]['message']['content']
    deepseek_evaluation = deepseek_response['choices'][0]['message']['content']

    return openai_evaluation, deepseek_evaluation