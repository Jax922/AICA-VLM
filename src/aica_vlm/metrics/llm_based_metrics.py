import json
import os
import re
import sys

from dotenv import load_dotenv
from openai import OpenAI


def get_openai_client():
    load_dotenv(override=True)

    api_key = os.getenv("OPENAI_API_KEY")
    base_url = os.getenv("OPENAI_BASE_URL")

    if not api_key or not base_url:
        print("Missing OPENAI_API_KEY or OPENAI_BASE_URL in your .env file.")
        print("Please create a .env file with the following format:\n")
        print("OPENAI_API_KEY=your-api-key")
        print("OPENAI_BASE_URL=https://your-base-url")
        sys.exit(1)

    return OpenAI(api_key=api_key, base_url=base_url)


def get_deepseek_client():
    load_dotenv(override=True)

    api_key = os.getenv("DEEPSEEK_API_KEY")
    base_url = os.getenv("DEEPSEEK_BASE_URL")
    if not api_key or not base_url:
        print("Missing DEEPSEEK_API_KEY or DEEPSEEK_BASE_URL in your .env file.")
        print("Please create a .env file with the following format:\n")
        print("DEEPSEEK_API_KEY=your-api-key")
        print("DEEPSEEK_BASE_URL=https://your-base-url")
        sys.exit(1)

    return OpenAI(api_key=api_key, base_url=base_url)


def evaluate_emotion_reasoning(vlm_output, gpt4o_reference):
    """
    Evaluate metrics for Emotion Reasoning tasks by comparing VLM output with GPT-4o reference.

    Args:
        vlm_output (str): The output generated by the VLM model.
        gpt4o_reference (str): The reference answer generated by GPT-4o.

    Returns:
        dict: A dictionary containing scores for emotional alignment, descriptiveness,
              and causal soundness.
    """
    prompt = f"""
    You are an evaluator specializing in Emotion Reasoning tasks. The VLM output explains why a specific emotion is associated with the input (e.g., an image). Compare the VLM output with the GPT-4o reference and evaluate the following metrics:

    1. Emotional Alignment (1-5): Does the generated content match the target emotion?
    2. Descriptiveness (1-5): How detailed and vivid is the generated content?
    3. Causal Soundness (1-5): Does the explanation logically support the emotional inference?

    VLM Output:
    {vlm_output}

    GPT-4o Reference:
    {gpt4o_reference}

    Provide your evaluation **strictly** in the following JSON format **without any explanation**:
    {{
        "emotional_alignment": <score>,
        "descriptiveness": <score>,
        "causal_soundness": <score>
    }}
    """

    openai_client = get_openai_client()
    deepseek_client = get_deepseek_client()

    openai_response = openai_client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "system",
                "content": "You are a helpful assistant for evaluating Emotion Reasoning tasks.",
            },
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                ],
            },
        ],
        temperature=0.7,
    )

    deepseek_response = deepseek_client.chat.completions.create(
        model="deepseek-chat",
        messages=[
            {
                "role": "system",
                "content": "You are a helpful assistant for evaluating Emotion Reasoning tasks.",
            },
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                ],
            },
        ],
        temperature=0.7,
    )

    openai_evaluation = openai_response.choices[0].message.content.strip()
    deepseek_evaluation = deepseek_response.choices[0].message.content.strip()
    return openai_evaluation, deepseek_evaluation


def evaluate_emotion_guided_generation(vlm_output, gpt4o_reference):
    """
    Evaluate metrics for Emotion-Guided Content Generation tasks by comparing VLM output with GPT-4o reference.

    Args:
        vlm_output (str): The output generated by the VLM model.
        gpt4o_reference (str): The reference answer generated by GPT-4o.

    Returns:
        dict: A dictionary containing scores for emotional alignment and descriptiveness.
    """
    prompt = f"""
    You are an evaluator specializing in Emotion-Guided Content Generation tasks. The VLM output generates content guided by a target emotion. Compare the VLM output with the GPT-4o reference and evaluate the following metrics:

    1. Emotional Alignment (1-5): Does the generated content match the target emotion?
    2. Descriptiveness (1-5): How detailed and vivid is the generated content?

    VLM Output:
    {vlm_output}

    GPT-4o Reference:
    {gpt4o_reference}

    Provide your evaluation **strictly** in the following JSON format **without any explanation**:
    {{
        "emotional_alignment": <score>,
        "descriptiveness": <score>
    }}
    """

    openai_client = get_openai_client()
    deepseek_client = get_deepseek_client()

    openai_response = openai_client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "system",
                "content": "You are a helpful assistant for evaluating Emotion-Guided Content Generation tasks.",
            },
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                ],
            },
        ],
        temperature=0.7,
    )
    deepseek_response = deepseek_client.chat.completions.create(
        model="deepseek-chat",
        messages=[
            {
                "role": "system",
                "content": "You are a helpful assistant for evaluating Emotion-Guided Content Generation tasks.",
            },
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                ],
            },
        ],
        temperature=0.7,
    )
    openai_evaluation = openai_response.choices[0].message.content.strip()
    deepseek_evaluation = deepseek_response.choices[0].message.content.strip()
    return openai_evaluation, deepseek_evaluation


def extract_json_from_response(text):
    match = re.search(r"\{.*?\}", text, re.DOTALL)
    if match:
        json_str = match.group()
        try:
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            print("[!] JSON Fail:", e)
            print("[Origin Content]:", json_str)
            return None
    else:
        print("[!] JSON not found in response:", text)
        print("[!]:", text)
        return None


def run(json_file, task_type):
    """
    Run the evaluation process for Emotion Reasoning or Emotion-Guided Content Generation tasks.
    Args:
        json_file (str): Path to the JSON file containing the task data.
        task_type (str): The type of task to evaluate. Must be either 'reasoning' or 'generation'.
    """

    from rich.console import Console
    from rich.progress import Progress

    console = Console()

    if task_type not in ["reasoning", "generation"]:
        raise ValueError(
            "Invalid task_type. Must be either 'reasoning' or 'generation'."
        )

    console.print(f"[bold green]Loading JSON file: {json_file}[/bold green]")

    # Load the JSON file
    with open(json_file, "r", encoding="utf-8") as f:
        data = json.load(f)

    total_count = len(data["results"])
    console.print(f"[bold blue]Total entries to process: {total_count}[/bold blue]")

    # Initialize averages
    if task_type == "reasoning":
        reasoning_average_openai = {
            "emotional_alignment": 0,
            "descriptiveness": 0,
            "causal_soundness": 0,
        }
        reasoning_average_deepseek = {
            "emotional_alignment": 0,
            "descriptiveness": 0,
            "causal_soundness": 0,
        }
    elif task_type == "generation":
        generation_average_openai = {
            "emotional_alignment": 0,
            "descriptiveness": 0,
        }
        generation_average_deepseek = {
            "emotional_alignment": 0,
            "descriptiveness": 0,
        }

    # Iterate through each entry in the JSON file with progress tracking
    with Progress() as progress:
        task = progress.add_task("[cyan]Processing entries...", total=total_count)

        for entry in data["results"]:
            vlm_output = entry.get("output_result", "")
            gpt4o_reference = entry.get("true_answer", "").strip()

            if task_type == "reasoning":
                # Evaluate Emotion Reasoning
                (
                    emotion_reasoning_results_openai,
                    emotion_reasoning_results_deepseek,
                ) = evaluate_emotion_reasoning(vlm_output, gpt4o_reference)
                emotion_reasoning_results_openai = extract_json_from_response(
                    emotion_reasoning_results_openai
                )
                emotion_reasoning_results_deepseek = extract_json_from_response(
                    emotion_reasoning_results_deepseek
                )

                reasoning_emotion_alignment_openai = emotion_reasoning_results_openai[
                    "emotional_alignment"
                ]
                reasoning_descriptiveness_openai = emotion_reasoning_results_openai[
                    "descriptiveness"
                ]
                reasoning_causal_soundness_openai = emotion_reasoning_results_openai[
                    "causal_soundness"
                ]

                reasoning_emotion_alignment_deepseek = (
                    emotion_reasoning_results_deepseek["emotional_alignment"]
                )
                reasoning_descriptiveness_deepseek = emotion_reasoning_results_deepseek[
                    "descriptiveness"
                ]
                reasoning_causal_soundness_deepseek = (
                    emotion_reasoning_results_deepseek["causal_soundness"]
                )

                # Update the entry with the evaluation results
                entry["emotion_reasoning"] = {
                    "openai": {
                        "emotional_alignment": reasoning_emotion_alignment_openai,
                        "descriptiveness": reasoning_descriptiveness_openai,
                        "causal_soundness": reasoning_causal_soundness_openai,
                    },
                    "deepseek": {
                        "emotional_alignment": reasoning_emotion_alignment_deepseek,
                        "descriptiveness": reasoning_descriptiveness_deepseek,
                        "causal_soundness": reasoning_causal_soundness_deepseek,
                    },
                }

                # Update the averages
                reasoning_average_openai[
                    "emotional_alignment"
                ] += reasoning_emotion_alignment_openai
                reasoning_average_openai[
                    "descriptiveness"
                ] += reasoning_descriptiveness_openai
                reasoning_average_openai[
                    "causal_soundness"
                ] += reasoning_causal_soundness_openai
                reasoning_average_deepseek[
                    "emotional_alignment"
                ] += reasoning_emotion_alignment_deepseek
                reasoning_average_deepseek[
                    "descriptiveness"
                ] += reasoning_descriptiveness_deepseek
                reasoning_average_deepseek[
                    "causal_soundness"
                ] += reasoning_causal_soundness_deepseek

            elif task_type == "generation":
                # Evaluate Emotion-Guided Content Generation
                (
                    emotion_generation_results_openai,
                    emotion_generation_results_deepseek,
                ) = evaluate_emotion_guided_generation(vlm_output, gpt4o_reference)
                emotion_generation_results_openai = extract_json_from_response(
                    emotion_generation_results_openai
                )
                emotion_generation_results_deepseek = extract_json_from_response(
                    emotion_generation_results_deepseek
                )

                generation_emotion_alignment_openai = emotion_generation_results_openai[
                    "emotional_alignment"
                ]
                generation_descriptiveness_openai = emotion_generation_results_openai[
                    "descriptiveness"
                ]

                generation_emotion_alignment_deepseek = (
                    emotion_generation_results_deepseek["emotional_alignment"]
                )
                generation_descriptiveness_deepseek = (
                    emotion_generation_results_deepseek["descriptiveness"]
                )

                # Update the entry with the evaluation results
                entry["emotion_guided_generation"] = {
                    "openai": {
                        "emotional_alignment": generation_emotion_alignment_openai,
                        "descriptiveness": generation_descriptiveness_openai,
                    },
                    "deepseek": {
                        "emotional_alignment": generation_emotion_alignment_deepseek,
                        "descriptiveness": generation_descriptiveness_deepseek,
                    },
                }

                # Update the averages
                generation_average_openai[
                    "emotional_alignment"
                ] += generation_emotion_alignment_openai
                generation_average_openai[
                    "descriptiveness"
                ] += generation_descriptiveness_openai
                generation_average_deepseek[
                    "emotional_alignment"
                ] += generation_emotion_alignment_deepseek
                generation_average_deepseek[
                    "descriptiveness"
                ] += generation_descriptiveness_deepseek

            # Update progress
            progress.update(task, advance=1)

    # Calculate the average scores
    if task_type == "reasoning":
        for key in reasoning_average_openai.keys():
            reasoning_average_openai[key] /= total_count
            reasoning_average_deepseek[key] /= total_count
    elif task_type == "generation":
        for key in generation_average_openai.keys():
            generation_average_openai[key] /= total_count
            generation_average_deepseek[key] /= total_count

    # Save the average scores to the JSON file
    if task_type == "reasoning":
        data["llm_based_metrics"] = {
            "emotion_reasoning": {
                "openai": reasoning_average_openai,
                "deepseek": reasoning_average_deepseek,
            }
        }
    elif task_type == "generation":
        data["llm_based_metrics"] = {
            "emotion_guided_generation": {
                "openai": generation_average_openai,
                "deepseek": generation_average_deepseek,
            }
        }

    # Save the updated JSON file to the original file
    with open(json_file, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=4, ensure_ascii=False)

    console.print(
        f"[bold green]Processing completed! Updated file saved to: {json_file}[/bold green]"
    )


if __name__ == "__main__":
    vlm_output = "The cat is happy because it is playing with a ball of yarn."
    gpt4o_reference = (
        "The cat's playful behavior indicates happiness, as it engages with the yarn."
    )

    (
        emotion_reasoning_results_openai,
        emotion_reasoning_results_deepseek,
    ) = evaluate_emotion_reasoning(vlm_output, gpt4o_reference)
    print("Emotion Reasoning Evaluation Results:")

    # parse json
    emotion_reasoning_results_openai = extract_json_from_response(
        emotion_reasoning_results_openai
    )
    print("======================================")
    print(emotion_reasoning_results_openai)
    reasoning_emotion_alignment_openai = emotion_reasoning_results_openai[
        "emotional_alignment"
    ]
    reasoning_descriptiveness_openai = emotion_reasoning_results_openai[
        "descriptiveness"
    ]
    reasoning_causal_soundness_openai = emotion_reasoning_results_openai[
        "causal_soundness"
    ]

    print("======================================")
    print(
        "openai_results",
        reasoning_emotion_alignment_openai,
        reasoning_descriptiveness_openai,
        reasoning_causal_soundness_openai,
    )

    (
        emotion_guided_generation_results_openai,
        emotion_guided_generation_results_deepseek,
    ) = evaluate_emotion_guided_generation(vlm_output, gpt4o_reference)
    print("Emotion-Guided Content Generation Evaluation Results:")
    print("======================================")
    # parse json
    emotion_guided_generation_results_openai = extract_json_from_response(
        emotion_guided_generation_results_openai
    )
    print(emotion_guided_generation_results_openai)
    generation_emotion_alignment_openai = emotion_guided_generation_results_openai[
        "emotional_alignment"
    ]
    generation_descriptiveness_openai = emotion_guided_generation_results_openai[
        "descriptiveness"
    ]
    print("======================================")
    print(
        "openai_results",
        generation_emotion_alignment_openai,
        generation_descriptiveness_openai,
    )
